<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 虚拟化 | Linux SA Notes By 枯木]]></title>
  <link href="http://kumu-Linux.github.io/blog/categories/xu-ni-hua/atom.xml" rel="self"/>
  <link href="http://kumu-Linux.github.io/"/>
  <updated>2014-02-20T16:24:47+08:00</updated>
  <id>http://kumu-Linux.github.io/</id>
  <author>
    <name><![CDATA[枯木]]></name>
    <email><![CDATA[1988.wulei@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[opennebula 3.8market兼容问题]]></title>
    <link href="http://kumu-Linux.github.io/blog/2014/02/20/opennebula-market/"/>
    <updated>2014-02-20T16:12:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2014/02/20/opennebula-market</id>
    <content type="html"><![CDATA[<p>今天下午发现公司所有的OpenNebula Sunstone访问出错，而且错误界面关闭不了，严重影响操作</p>

<center><img src="http://kumu-Linux.github.io/images/OpenNebula/OpenNebula_market_err.png" /></center>




<!--more-->


<p>Firebug获知锁定错误为market插件</p>

<center><img src="http://kumu-Linux.github.io/images/OpenNebula/OpenNebula_market_err2.png" /></center>


<p>Google获知原因所在，具体问题可以参见<a href="http://www.marshut.com/iqmmzn/opennebula-3-8-and-market-compatiable-problem.html">opennebula-3-8-and-market-compatiable-problem</a> ，解决方法是关闭sunstone的Market plugins，修改<code>etc/sunstone-plugins.yaml</code>配置文件，修改下行从<code>True</code>变为<code>false</code>，如下：
``` ruby
- plugins/marketplace-tab.js:</p>

<pre><code>:ALL: false
:user:
</code></pre>

<p>:group:
```</p>

<p>完成之后刷新页面即可恢复正常</p>

<p>--EOF--</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenNebula4.4入门之安装和节点配置]]></title>
    <link href="http://kumu-Linux.github.io/blog/2013/12/19/opennebula4-dot-4/"/>
    <updated>2013-12-19T10:51:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2013/12/19/opennebula4-dot-4</id>
    <content type="html"><![CDATA[<p>OpenNebula入门的PDF文档已可下载，<a href="http://vdisk.weibo.com/s/EKoLFfHiE-oT/1387002741">OpenNebula4.4入门配置</a>，本博客连载更新相关内容</p>

<!--more-->


<p><strong>本文内容目录组成如下</strong>：</p>

<ul>
<li><a href="#env">环境说明</a></li>
<li><a href="#soft">软件包组成</a></li>
<li><a href="#server">Server端安装和配置</a></li>
<li><a href="#node_server">节点端安装配置</a></li>
<li><a href="#node_add">添加节点</a>

<ul>
<li><a href="#onehost">onehost</a></li>
</ul>
</li>
</ul>


<h2 id="env">环境说明</h2>


<p>因为CentOS6.4虚拟化有很大的一个提升，所以系统环境管理端和节点宿主机都采用CentOS6.4 x86_64</p>

<h2 id="soft">软件包组成</h2>


<p>从OpenNebula官网下载<a href="http://downloads.opennebula.org/packages/opennebula-4.4.0/CentOS-6/CentOS-6-opennebula-4.4.0-1.tar.gz">CentOS/RHEL 6</a>对应软件包或者加入OpenNebula源，直接下载软件包这里不再赘述，添加OpenNebula源方法如下：
``` bash</p>

<h1>cat &lt;&lt; EOT > /etc/yum.repos.d/opennebula.repo</h1>

<p>[opennebula]
name=opennebula
baseurl=http://downloads.opennebula.org/repo/CentOS/6/stable/\$basearch
enabled=1
gpgcheck=0
EOT
```</p>

<p>OpenNebula4.4主要有以下几个软件组成：
``` bash</p>

<h1>ls opennebula-*</h1>

<p>opennebula-4.4.0-1.x86_64.rpm           //OpenNebula命令行指令
opennebula-flow-4.4.0-1.x86_64.rpm      //管理OpenNebula服务
opennebula-java-4.4.0-1.x86_64.rpm      //OpenNebula Java Api
opennebula-ozones-4.4.0-1.x86_64.rpm    //OpenNebula网页使用界面
opennebula-server-4.4.0-1.x86_64.rpm    //OpenNebula Server守护进程
opennebula-common-4.4.0-1.x86_64.rpm    //基本依赖性组件
opennebula-gate-4.4.0-1.x86_64.rpm      //使虚拟机和OpenNebula之间的通信
opennebula-node-kvm-4.4.0-1.x86_64.rpm  //元软件包，包括安装oneadmin用户、libvirt和kvm
opennebula-ruby-4.4.0-1.x86_64.rpm      //ruby依赖性组件
opennebula-sunstone-4.4.0-1.x86_64.rpm  //OpenNebula网页使用界面
opennebula-context-4.4.0-1.x86_64.rpm   //context组件
```</p>

<h2 id="server">Server端安装和配置</h2>


<p>为解决一些依赖关系，安装之前可以激活epel源，因为测试为CentOS6.4，因此激活方式如下：
``` bash</p>

<h1>rpm -ivh http://dl.fedoraproject.org/pub/epel/6Server/x86_64/epel-release-6-8.noarch.rpm</h1>

<p>```</p>

<p>如果下载的是OpenNebula软件包，则进入解压目录，安装方式如下 [以下安装为组成Server端最基本的软件]：
``` bash</p>

<h1>yum localinstall opennebula-server-4.4.0-1.x86_64.rpm  \</h1>

<p>opennebula-4.4.0-1.x86_64.rpm opennebula-common-4.4.0-1.x86_64.rpm \
opennebula-ruby-4.4.0-1.x86_64.rpm opennebula-sunstone-4.4.0-1.x86_64 -y
```</p>

<p>如果使用OpenNebula的源，安装如下：
``` bash</p>

<h1>yum install opennebula-server opennebula-sunstone -y</h1>

<p>```</p>

<p>安装完成之后创建如下用户以及目录文件：
``` bash</p>

<h1>grep oneadmin /etc/passwd</h1>

<p>oneadmin:x:9869:9869::/var/lib/one:/bin/bash</p>

<h1>ls -ld /etc/one/  //OpenNebula相关配置文件所在目录</h1>

<p>drwxr-x---. 11 root oneadmin 4096 Aug 20 11:35 /etc/one/</p>

<h1>ls /etc/init.d/opennebula*</h1>

<p>/etc/init.d/opennebula  /etc/init.d/opennebula-occi  /etc/init.d/opennebula-sunstone</p>

<h1>ls -ld /var/log/one/</h1>

<p>drwxr-x---. 2 oneadmin oneadmin 4096 Jul 25 01:13 /var/log/one/
```</p>

<p>默认OpenNebula数据存储使用sqlite，如果需要使用MySQL，则需要做如下操作：<br/>
<strong>1.</strong>  创建相关数据库：
``` bash
mysql> create database opennebula;
Query OK, 1 row affected (0.00 sec)</p>

<p>mysql> grant all privileges on opennebula.* to oneadmin@'localhost' identified by 'oneadmin';
Query OK, 0 rows affected (0.00 sec)</p>

<p>mysql> flush privileges;
Query OK, 0 rows affected (0.00 sec)
```</p>

<p><strong>2.</strong>  修改配置文件如下 [用户、端口、密码、库名和实际情况对应修改]：
``` bash</p>

<h1>vim /etc/one/oned.conf</h1>

<p>… …</p>

<h1>DB = [ backend = "sqlite" ]</h1>

<h1>Sample configuration for MySQL</h1>

<p>DB = [ backend = "mysql",</p>

<pre><code>   server  = "localhost",
   port    = 3306,
   user    = "oneadmin", 
   passwd  = "oneadmin", 
   db_name = "opennebula" ]
</code></pre>

<p>… …
```</p>

<p>修改sunstone默认监听IP:
``` bash</p>

<h1>grep ':host' /etc/one/sunstone-server.conf</h1>

<p>:host: 127.0.0.1</p>

<h1>sed -i '/:host/s/127.0.0.1/192.168.80.130/g' /etc/one/sunstone-server.conf</h1>

<h1>grep ':host' /etc/one/sunstone-server.conf</h1>

<p>:host: 192.168.80.130
```</p>

<p>启动相关服务:
``` bash</p>

<h1>/etc/init.d/opennebula start</h1>

<h1>/etc/init.d/opennebula-sunstone start</h1>

<h1>lsof -i:9869</h1>

<p>COMMAND   PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
ruby    22266 oneadmin    6u  IPv4 106746      0t0  TCP 192.168.80.130:9869 (LISTEN)
```</p>

<p>修改datastore:</p>

<p>OpenNebula默认用的是Shared Transfer Driver，这种模式比较适合快速部署和热迁移，只是要配置网络文件系统。如果没有网络文件系统，不想做热迁移，那么可以换成SSH Transfer Driver测试部署。
<code>bash
$ onedatastore list
  ID NAME                SIZE AVAIL CLUSTER      IMAGES TYPE DS       TM      
   0 system                0M -     -                 0 sys  -        shared
   1 default            98.4G 85%   -                 1 img  fs       shared
   2 files              98.4G 85%   -                 0 fil  fs       ssh
$ onedatastore update 1
CLONE_TARGET="SYSTEM"
DISK_TYPE="FILE"
DS_MAD="fs"
LN_TARGET="SYSTEM"
TM_MAD="ssh"
TYPE="IMAGE_DS"
$ onedatastore list
  ID NAME                SIZE AVAIL CLUSTER      IMAGES TYPE DS       TM      
   0 system                0M -     -                 0 sys  -        shared
   1 default            98.4G 85%   -                 1 img  fs       ssh
   2 files              98.4G 85%   -                 0 fil  fs       ssh
</code></p>

<p>修改过程产生如下错误：
<code>bash
$ onedatastore update 1
Editor not defined
</code></p>

<p>这是因为如下原因，CentOS默认vi位置是/bin/vi，添加相关链接即可
``` bash</p>

<h1>grep -i editor_path= /usr/lib/one/ruby/cli/one_helper.rb</h1>

<p>EDITOR_PATH='/usr/bin/vi'</p>

<h1>ln -s /bin/vi /usr/bin/vi</h1>

<p>```</p>

<p>用户名和密码通过以下方式获得：
``` bash</p>

<h1>cat /var/lib/one/.one/one_auth</h1>

<p>oneadmin:cd24c3a59c9fd8a7ab853b10247e8147
```</p>

<p><strong>注</strong>：测试过程中因为测试环境服务端时间不对，导致cookie被忽略，OpenNebula Sunstone选择Keep me logged in一直登陆不上或者直接登陆很快退出，寻找原因花了很长时间，最后调整到正确时间，登陆显示ok。P.S: 时间是一个非常容易被我们忽略的问题，切记切记!</p>

<p>完成以上步骤之后，浏览器登陆 <a href="http://ip:9869">http://ip:9869</a> 即可</p>

<h2 id="node_server">节点端安装配置</h2>


<p>软件包下载见Server端安装章节，节点只需要安装以下两个软件</p>

<pre><code>opennebula-node-kvm-4.4.0-1.x86_64.rpm  
opennebula-common-4.4.0-1.x86_64.rpm
</code></pre>

<p>yum安装以上软件即可，安装过程同时会安装虚拟化相关组件，包括bridge-utils、libvirt、qemu-kvm、qemu-img等。</p>

<p>桥接网络：
``` bash</p>

<h1>cat /etc/sysconfig/network-scripts/ifcfg-eth0</h1>

<p>DEVICE=eth0
TYPE=Ethernet
ONBOOT=yes
BRIDGE=br0
NAME="System eth0"</p>

<h1>cat /etc/sysconfig/network-scripts/ifcfg-br0</h1>

<p>DEVICE=br0
ONBOOT=yes
TYPE=Bridge
BOOTPROTO=static
IPADDR=192.168.80.131
NETMASK=255.255.255.0
GATEWAY=192.168.80.2
```</p>

<p>修改之后，重启网络并查看确认：
``` bash</p>

<h1>service network restart</h1>

<h1>brctl show</h1>

<p>bridge name bridge id       STP enabled interfaces
br0     8000.000c2942e561   no      eth0
```</p>

<p>修改/etc/libvirt/qemu.conf的相关配置：
<code>bash
user  = "oneadmin"
group = "oneadmin"
dynamic_ownership = 0
</code></p>

<p>修改/etc/libvirt/libvirtd.conf相关配置：
<code>bash
listen_tcp = 1          //OpenNebula使用libvirt提供的TCP协议
listen_tls = 0
</code></p>

<p>修改/etc/sysconfig/libvirtd开启监听选项：
<code>bash
LIBVIRTD_ARGS="--listen"
</code></p>

<p>启动libvirtd服务：
``` bash</p>

<h1>/etc/init.d/libvirtd start</h1>

<h1>netstat -tulnp | grep libvirt</h1>

<p>tcp        0      0 0.0.0.0:16509               0.0.0.0:*                   LISTEN      2664/libvirtd
```</p>

<p>ssh无密码登陆：</p>

<p>ssh使用公钥认证无密码登陆这个比较简单，顺带也提一下，方法如下：</p>

<p><strong>管理端</strong>
``` bash</p>

<h1>su - oneadmin</h1>

<p>$ cat ~/.ssh/config     //增加超时时间，不询问直接添加主机到known_hosts文件
ConnectTimeout 5
Host *</p>

<pre><code>StrictHostKeyChecking no
UserKnownHostsFile /dev/null
</code></pre>

<p>```</p>

<p><strong>节点端</strong>
``` bash</p>

<h1>su - oneadmin</h1>

<p>$ vim .ssh/authorized_keys          //把管理端ssh公钥加入节点.ssh/authorized_keys文件
$ chmod 400 .ssh/authorized_keys
```</p>

<p>如此，Server端的oneadmin用户就可以无密码登陆节点oneadmin了。</p>

<h2 id="node_add">添加节点</h2>


<p>节点如此安装软件和配置之后便可以在Server端添加了，可以使用web添加，也可以使用命令添加。关于web界面的添加可以参考本人共享的pdf文档，这里不作具体的介绍，只介绍命令添加。</p>

<h3 id="onehost">onehost命令</h3>


<p>使用命令行添加主机也比较简单，这里使用的命令是<strong>onehost</strong></p>

<p>使用onehost命令删除之前web创建的主机，如下：
<code>bash
$ su - oneadmin
$ onehost list
  ID NAME            CLUSTER   RVM      ALLOCATED_CPU      ALLOCATED_MEM STAT  
   1 192.168.80.131  -           0       0 / 400 (0%)     0K / 3.7G (0%) on    
$ onehost delete 1      //删除主机，可以是ID也可以是NAME
$ onehost list
  ID NAME            CLUSTER   RVM      ALLOCATED_CPU      ALLOCATED_MEM static
</code></p>

<p>当然删除之后我们还是需要再创建一遍，虽然很无聊，But你懂的，如下
<code>bash
$ onehost create 192.168.80.131 --im kvm --vm kvm --net dummy
ID: 2
$ onehost list
  ID NAME            CLUSTER   RVM      ALLOCATED_CPU      ALLOCATED_MEM STAT  
   2 192.168.80.131  -           0       0 / 400 (0%)     0K / 3.7G (0%) on    
</code></p>

<pre><code>--im/-i:信息管理driver. 可选: kvm, xen, vmware, ec2, ganglia, dummy.
--vm/-v: 虚拟化管理driver. 可选: kvm, xen, vmware, ec2, dummy.
--net/-n: 虚拟网络driver. 可选: 802.1Q,dummy,ebtables,fw,ovswitch,vmware.
</code></pre>

<p>查看主机的详细信息 <strong>onehost show</strong>
```
$ onehost show 2
HOST 2 INFORMATION                                                            <br/>
ID                    : 2                 <br/>
NAME                  : 192.168.80.131    <br/>
CLUSTER               : -                 <br/>
STATE                 : MONITORED         <br/>
IM_MAD                : kvm               <br/>
VM_MAD                : kvm               <br/>
VN_MAD                : dummy             <br/>
LAST MONITORING TIME  : 11/29 22:19:21</p>

<p>HOST SHARES                                                                   <br/>
TOTAL MEM             : 3.7G              <br/>
USED MEM (REAL)       : 111M              <br/>
USED MEM (ALLOCATED)  : 0K                <br/>
TOTAL CPU             : 400               <br/>
USED CPU (REAL)       : 0                 <br/>
USED CPU (ALLOCATED)  : 0                 <br/>
RUNNING VMS           : 0</p>

<p>… …
```</p>

<p>通过-x选项还可以以xml的格式显示主机相关信息
```
$ onehost show -x 2
<HOST>
  <ID>2</ID>
  <NAME>192.168.80.131</NAME>
  <STATE>2</STATE>
  <IM_MAD>kvm</IM_MAD>
  <VM_MAD>kvm</VM_MAD>
  <VN_MAD>dummy</VN_MAD>
  <LAST_MON_TIME>1385735001</LAST_MON_TIME>
  <CLUSTER_ID>-1</CLUSTER_ID>
  <CLUSTER/>
  <HOST_SHARE></p>

<pre><code>&lt;DISK_USAGE&gt;0&lt;/DISK_USAGE&gt;
&lt;MEM_USAGE&gt;0&lt;/MEM_USAGE&gt;
&lt;CPU_USAGE&gt;0&lt;/CPU_USAGE&gt;
&lt;MAX_DISK&gt;0&lt;/MAX_DISK&gt;
&lt;MAX_MEM&gt;3916984&lt;/MAX_MEM&gt;
&lt;MAX_CPU&gt;400&lt;/MAX_CPU&gt;
&lt;FREE_DISK&gt;0&lt;/FREE_DISK&gt;
&lt;FREE_MEM&gt;3803128&lt;/FREE_MEM&gt;
&lt;FREE_CPU&gt;399&lt;/FREE_CPU&gt;
&lt;USED_DISK&gt;0&lt;/USED_DISK&gt;
&lt;USED_MEM&gt;113856&lt;/USED_MEM&gt;
&lt;USED_CPU&gt;0&lt;/USED_CPU&gt;
&lt;RUNNING_VMS&gt;0&lt;/RUNNING_VMS&gt;
</code></pre>

<p>  </HOST_SHARE>
… …
```</p>

<p>onehost还有两个选项,disable和enable，disable表示不再监控该物理主机，但是不影响正在运行的虚拟机，enable表示开启监控
<code>bash
$ onehost disable 0
$ onehost enable 0
</code></p>

<p>--EOF--</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[qemu-kvm桥接网络]]></title>
    <link href="http://kumu-Linux.github.io/blog/2013/12/10/kvm-tap/"/>
    <updated>2013-12-10T22:22:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2013/12/10/kvm-tap</id>
    <content type="html"><![CDATA[<!--more-->


<h2>手动桥接</h2>

<p>qemu-kvm安装或者启动虚拟系统的时候如果需要和外界通信，那么就要设置网络桥接
<code>bash
/usr/libexec/qemu-kvm -m 1024 \
-drive file=/data/images/CentOS6_4.qcow2,if=virtio \
-net nic,model=virtio -net tap,script=no -nographic -vnc :0
</code></p>

<p>使用<code>-net tap,script=no</code>方式启动之后，系统会生成tapX的虚拟网卡,默认是DOWN状态的
``` bash</p>

<h1>ip link show dev tap0</h1>

<p>37: tap0: &lt;BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 500</p>

<pre><code>link/ether d2:b0:af:7b:23:0f brd ff:ff:ff:ff:ff:ff
</code></pre>

<p>```</p>

<p>如果想和外界通信，可以手动执行生效，如下所示当前与br0桥接的设备，并没有tap相关的网卡
``` bash</p>

<h1>brctl show br0</h1>

<p>bridge name bridge id       STP enabled interfaces
br0     8000.b8975a626020   no      eth0</p>

<pre><code>                        vnet0
                        vnet1
</code></pre>

<p>```</p>

<p>我们需要把tap0也桥接到br0下以便和外界通信，方法如下
``` bash</p>

<h1>ip link set tap0 up       //使tap0状态变为up</h1>

<h1>brctl addif br0 tap0      //桥接tap0到br0</h1>

<h1>brctl show br0</h1>

<p>bridge name bridge id       STP enabled interfaces
br0     8000.b8975a626020   no      eth0</p>

<pre><code>                        tap0
                        vnet0
                        vnet1
</code></pre>

<p>```</p>

<p><code>brctl delif br0 tap0</code>删除桥接网络，qemu-kvm工具在客户机关闭时会自动解除TAP设备的bridge绑定，所以这一步无需操作</p>

<h2>脚本实现</h2>

<p><code>bash
/usr/libexec/qemu-kvm -m 1024 \
-drive file=/data/images/CentOS6_4.qcow2,if=virtio \
-net nic,model=virtio -net tap,script=/tmp/qemu-ifup.sh -nographic -vnc :0
</code></p>

<p>如上<code>tap,script=/tmp/qemu-ifup.sh</code>指定script网络配置启动前启动脚本，脚本内容如下
``` bash</p>

<h1>cat /tmp/qemu-ifup.sh</h1>

<h1>!/bin/bash</h1>

<h1>桥接网络设备</h1>

<p>switch=br0</p>

<p>if [ -n $1 ]; then          //$1为qemu-kvm传递值，这里是tap</p>

<pre><code>ip link set $1 up
brctl addif ${switch} $1
exit 0
</code></pre>

<p>else
   echo "no interface!"
   exit 1
fi
```
如此，便不需要每次手动添加了</p>

<p>这部分内容的理解主要是 <a href="http://smilejay.com/2012/08/kvm-bridge-networking/">KVM使用网桥模式</a> 这篇文章，顺便推荐此博主的《KVM虚拟化技术：实战与原理解析》一书，对系统的学习KVM很有帮助</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenNebula添加节点]]></title>
    <link href="http://kumu-Linux.github.io/blog/2013/11/07/opennubula-node/"/>
    <updated>2013-11-07T15:14:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2013/11/07/opennubula-node</id>
    <content type="html"><![CDATA[<h3>需求</h3>

<p>The hosts must have a working installation of KVM, that usually requires:</p>

<ul>
<li>CPU with VT extensions</li>
<li>libvirt >= 0.4.0</li>
<li>kvm kernel modules (kvm.ko, kvm-{intel,amd}.ko). Available from kernel 2.6.20 onwards.</li>
<li>the qemu user-land tools</li>
</ul>


<!--more-->


<p>笔者测试所用为VMware Workstation，除本身物理机支持并开启虚拟化外，Workstation也要开启相关配置[ 设置--处理器，查看是否开启 ]</p>

<h3>节点软件安装</h3>

<p>可以参见<a href="http://kumu-linux.github.io/blog/2013/08/22/opennebula-install/">OpenNebula在CentOS6.4安装备忘</a></p>

<p>``` bash</p>

<h1>yum install qemu-kvm qemu-img libvirt ruby \</h1>

<p>libvirt-python python-virtinst libvirt-client</p>

<h1>yum install opennebula-common-4.2.0-1.x86_64.rpm \</h1>

<p>opennebula-node-kvm-4.2.0-1.x86_64.rpm
```</p>

<h3>节点配置</h3>

<h4>启动kvm</h4>

<p>``` bash</p>

<h1>/etc/init.d/libvirtd start</h1>

<p>```</p>

<h4>桥接网络</h4>

<p>``` bash</p>

<h1>yum install bridge-utils -y</h1>

<p>```</p>

<p><strong>桥接实例</strong>:</p>

<p>``` bash</p>

<h1>cat /etc/sysconfig/network-scripts/ifcfg-eth0</h1>

<p>DEVICE=eth0
TYPE=Ethernet
ONBOOT=yes
NAME="System eth0"
BRIDGE="br0"</p>

<h1>cat /etc/sysconfig/network-scripts/ifcfg-br0</h1>

<p>DEVICE="br0"
TYPE="Bridge"  # 注意大小写
BOOTPROTO="static"
IPADDR=192.168.80.131
NETMASK=255.255.255.0
GATEWAY=192.168.80.2
ONBOOT="yes"
DELAY=0
```</p>

<p>修改完毕，重启网络</p>

<h4>相关配置修改</h4>

<p>修改/etc/libvirt/qemu.conf的相关配置：
``` bash</p>

<h1>grep -vE '<sup>($|#)'</sup> /etc/libvirt/qemu.conf</h1>

<p>user  = "oneadmin"
group = "oneadmin"
dynamic_ownership = 0
```</p>

<p>修改/etc/libvirt/libvirtd.conf相关配置：
<code>bash
listen_tcp = 1
listen_tls = 0
mdns_adv = 0
unix_sock_group = "oneadmin"
unix_sock_ro_perms = "0777"
unix_sock_rw_perms = "0777"
auth_unix_ro = "none"
auth_unix_rw = "none"
</code></p>

<p>修改/etc/sysconfig/libvirtd相关配置：
<code>bash
LIBVIRTD_ARGS="--listen"
</code></p>

<p>启动libvirtd服务[安全起见可以只监听内网IP]：
``` bash</p>

<h1>/etc/init.d/libvirtd restart</h1>

<h1>netstat -tulnp | grep libvirtd</h1>

<p>tcp        0      0 0.0.0.0:16509       0.0.0.0:<em>       LISTEN      50818/libvirtd    <br/>
tcp        0      0 :::16509            :::</em>            LISTEN      50818/libvirtd    <br/>
```</p>

<p>修改 /etc/sudoers 文件，最后一行加上：
<code>bash
oneadmin ALL=(root)NOPASSWD:ALL
</code></p>

<p>CentOS系统的sudo选项requiretty是默认打开的，远程执行命令时，ssh默认不会分配tty。没有tty，sudo就无法在获取密码时关闭回显。使用-tt选项强制SSH分配tty（使用两次-tt）。另一方面，sudoers中的Defaults选项requiretty要求只有拥有tty的用户才能使用sudo。可以通过visudo编辑配置文件，禁用这个选项：
``` bash</p>

<h1>Defaults    requiretty</h1>

<p>```</p>

<p>添加<code>oneadmin</code>用户和OpenNebula Server主机ssh公钥认证，使得OpenNebula Server主机<code>oneadmin</code>用户可以使用公钥无密码登陆，关于ssh密钥配置这里不再进一步说明。</p>

<h4>其它配置</h4>

<p>另外OpenNebula的脚本要用到/sbin/brctl，而CentOS的路径是/usr/sbin/brctl，添加软链接：
<code>bash
ln -s /usr/sbin/brctl /sbin/brctl
</code></p>

<p>还要用到/usr/bin/kvm，而CentOS没有链接，需要设置软链接：
<code>bash
ln -s /usr/libexec/qemu-kvm /usr/bin/kvm
</code></p>

<p>最后在Web上添加host主机节点即可，状态显示为<code>on</code>则表示添加成功。</p>

<p>--EOF--</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenNebula KVM磁盘热插拔]]></title>
    <link href="http://kumu-Linux.github.io/blog/2013/09/25/kvm-disk-hotplug/"/>
    <updated>2013-09-25T15:22:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2013/09/25/kvm-disk-hotplug</id>
    <content type="html"><![CDATA[<p>KVM支持以下两种磁盘类型的热插拔：</p>

<ul>
<li>sd: SCSI (default).</li>
<li>vd: virtio.</li>
</ul>


<p>KVM虚拟机需要开启acpi才支持磁盘的热插拔，使用OpenNebula安装虚拟机的时候需要设置如下选项：</p>

<ul>
<li>FEATURES = [ acpi="yes" ]</li>
</ul>


<!--more-->


<h3>virtio磁盘热插拔</h3>

<p>虚拟机加载acpiphp驱动</p>

<p>``` bash</p>

<h1>modprobe acpiphp  # 加载驱动</h1>

<h1>fdisk -l</h1>

<p>Disk /dev/vda: 32.2 GB, 32212254720 bytes
255 heads, 63 sectors/track, 3916 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes</p>

<p>   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *           1          13      104391   83  Linux
/dev/vda2              14        3916    31350847+  8e  Linux LVM
```</p>

<p>选择需要添加磁盘的虚拟机- [ Disks &amp; Hotplugging ] ，配置完成之后选择 Attach 挂载使用</p>

<center><img src="http://kumu-Linux.github.io/images/OpenNebula/OpenNebula_disk1.jpg" width="500"></center>


<p>Device Prefix选择sd表示scsi磁盘，如果是vd则是vitio类型磁盘。选择Attach之后，刷新页面就会看到新建的磁盘。</p>

<center><img src="http://kumu-Linux.github.io/images/OpenNebula/OpenNebula_disk2.jpg" width="500"></center>


<p>此时查看vda磁盘是否生效
``` bash</p>

<h1>fdisk -l          # 查看磁盘识别</h1>

<p>...</p>

<p>Disk /dev/vdb: 1048 MB, 1048576000 bytes
16 heads, 63 sectors/track, 2031 cylinders
Units = cylinders of 1008 * 512 = 516096 bytes</p>

<p>Disk /dev/vdb doesn't contain a valid partition table
```</p>

<h3>scsi磁盘热插拔</h3>

<p>虚拟机加载acpiphp驱动</p>

<p>``` bash</p>

<h1>modprobe acpiphp  # 加载驱动</h1>

<p>```</p>

<p>选择需要添加磁盘的虚拟机- [ Disks &amp; Hotplugging ] ，配置完成之后选择 Attach挂载使用</p>

<center><img src="http://kumu-Linux.github.io/images/OpenNebula/OpenNebula_disk3.jpg" width="500"></center>


<p>如下即可看到新添加的磁盘
``` bash</p>

<h1>fdisk -l</h1>

<p>...</p>

<p>Disk /dev/sda: 1048 MB, 1048576000 bytes
33 heads, 61 sectors/track, 1017 cylinders
Units = cylinders of 2013 * 512 = 1030656 bytes</p>

<p>Disk /dev/sda doesn't contain a valid partition table
```</p>

<p>如果未识别scsi磁盘，执行如下命令，使得KVM虚拟机识别
``` bash</p>

<h1>echo '- - -' > /sys/class/scsi_host/host0/scan</h1>

<h1>fdisk -l</h1>

<p>...
Disk /dev/sdb: 1048 MB, 1048576000 bytes
33 heads, 61 sectors/track, 1017 cylinders
Units = cylinders of 2013 * 512 = 1030656 bytes</p>

<p>Disk /dev/sdb doesn't contain a valid partition table
```</p>

<h3>参考文档</h3>

<ul>
<li><a href="http://opennebula.org/documentation:rel4.2:kvmg">KVM Driver 4.2</a></li>
<li><a href="http://serverfault.com/questions/453456/adding-virtio-block-devices-at-runtime-in-libvirt-kvm">Adding Virtio block devices at runtime in Libvirt KVM</a></li>
</ul>


<p>--EOF--</p>
]]></content>
  </entry>
  
</feed>
