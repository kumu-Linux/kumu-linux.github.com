<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 虚拟化 | OPS Notes By 枯木]]></title>
  <link href="http://kumu-Linux.github.io/blog/categories/xu-ni-hua/atom.xml" rel="self"/>
  <link href="http://kumu-Linux.github.io/"/>
  <updated>2014-05-28T12:00:52+08:00</updated>
  <id>http://kumu-Linux.github.io/</id>
  <author>
    <name><![CDATA[枯木]]></name>
    <email><![CDATA[1988.wulei@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Openebula镜像制作]]></title>
    <link href="http://kumu-Linux.github.io/blog/2014/04/25/openebula-image/"/>
    <updated>2014-04-25T20:45:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2014/04/25/openebula-image</id>
    <content type="html"><![CDATA[<p><code>qemu-img</code>和<code>qemu-kvm</code>命令是制作系统镜像的重要工具，在介绍这两个工具之前，稍微简单说明下虚拟机镜像格式，目前虚拟机有多种镜像格式可供选择，常见的有如<code>raw</code>、<code>vdi</code>、<code>qcow2</code>、<code>vmdk</code>、<code>qed</code>、<code>vhd</code>等格式。</p>

<!--more-->


<h2>qemu-img</h2>

<p><code>qemu-img</code>是QEMU的磁盘管理工具，支持多种虚拟镜像格式</p>

<p><code>bash
$ qemu-img -h | grep Supported
Supported formats: raw cow qcow vdi vmdk cloop dmg bochs vpc vvfat
qcow2 qed parallels nbd blkdebug host_cdrom host_floppy host_device file
</code></p>

<p><code>qemu-img</code>默认创建的格式是<code>raw</code>，man手册中对几种格式也都有介绍。以下为对raw和qcow2镜像的详细介绍：</p>

<h3>raw</h3>

<p>原始的磁盘镜像格式，qemu-img默认支持的格式，它的优势在于它非常简单而且非常容易移植到其他模拟器（emulator，QEMU 也是一个emulator）上去使用。如果客户机文件系统（如Linux上的ext2/ext3/ext4、Windows的NTFS）支持“空洞” （hole），那么镜像文件只有在被写有数据的扇区才会真正占用磁盘空间，从而有节省磁盘空间的作用。<code>qemu-img</code>默认的<code>raw</code>格式的文件其实是稀疏文件（sparse file）[<em>稀疏文件就是在文件中留有很多空余空间，留备将来插入数据使用。如果这些空余空间被ASCII码的NULL字符占据，并且这些空间相当大，那么，这个文件就被称为稀疏文件，而且，并不分配相应的磁盘块。</em>]，dd命令创建的也是raw格式，不过dd一开始就让镜像实际占用了分配的空间，而没有使用稀疏文件的方式对待空洞而节省磁盘空间。尽管一开始就实际占用磁盘空间的方式没有节省磁盘的效果，不过它在写入新的数据时不需要宿主机从现有磁盘空间中分配，从而在第一次写入数据时性能会比稀疏文件的方式更好一点。简单来说，raw有以下几个特点：</p>

<ul>
<li>寻址简单，访问效率高</li>
<li>可以通过格式转换工具方便地转换为其它格式</li>
<li>格式实现简单，不支持压缩、快照和加密</li>
<li>能够直接被宿主机挂载，不用开虚拟机即可在宿主和虚拟机间进行数据传输</li>
</ul>


<h3>qcow2</h3>

<p><code>qcow2</code>是qcow的一种改进，是Qemu实现的一种虚拟机镜像格式。更小的虚拟硬盘空间（尤其是宿主分区不支持hole的情况下），支持压缩、加密、快照功能，磁盘读写性能较raw差。</p>

<h3>qemu-img它支持的命令分为如下几种</h3>

<ul>
<li>（1）check [-f fmt] filename</li>
</ul>


<p>对磁盘镜像文件进行一致性检查，查找镜像文件中的错误，目前仅支持对<code>“qcow2”</code>、<code>“qed”</code>、<code>“vdi”</code>格式文件的检查。其中，<code>qcow2</code>是 QEMU 0.8.3版本引入的镜像文件格式，也是目前使用最广泛的格式。<code>qed</code>（QEMU enhanced disk）是从QEMU 0.14版开始加入的增强磁盘文件格式，为了避免qcow2格式的一些缺点，也为了提高性能，不过目前还不够成熟。而<code>vdi</code>（Virtual Disk Image）是Oracle的VirtualBox虚拟机中的存储格式。参数-f fmt是指定文件的格式，如果不指定格式<code>qemu-img</code>会自动检测，<code>filename</code>是磁盘镜像文件的名称（包括路径）。</p>

<p><code>bash
$ qemu-img check CentOS6.4-x86_64.qcow2
No errors were found on the image.
</code></p>

<ul>
<li>（2）create [-f fmt] filename [size]</li>
</ul>


<p>创建一个格式为fmt大小为size文件名为filename的镜像文件。</p>

<p><code>bash
$ qemu-img create -f qcow2 test.qcow2 10G
Formatting 'test.qcow2', fmt=qcow2 size=10737418240 encryption=off cluster_size=65536
$ qemu-img create -f qcow2 test.raw 10G
Formatting 'test.raw', fmt=qcow2 size=10737418240 encryption=off cluster_size=65536
</code></p>

<blockquote><p><strong>注意</strong>：这里的qcow2后缀只是为了便于自己区分格式方便，如果不加后缀也可以通过qemu-img来获取镜像的格式。</p></blockquote>

<ul>
<li>（3）info [-f fmt] filename</li>
</ul>


<p>显示filename镜像文件的信息。如果文件是使用稀疏文件的存储方式，也会显示出它的本来分配的大小以及实际已占用的磁盘空间大小。如果文件中存放有客户机快照，快照的信息也会被显示出来。</p>

<p><code>bash
$ qemu-img info test.qcow2
image: test.qcow2
file format: qcow2
virtual size: 10G (10737418240 bytes)
disk size: 136K
cluster_size: 65536
$ qemu-img info test.raw    //qemu-img生成raw格式镜像也是采用稀疏文件方式存储的
image: test.raw
file format: qcow2
virtual size: 10G (10737418240 bytes)
disk size: 136K
cluster_size: 65536
$ dd &lt;/dev/zero &gt;test.dd bs=1MB count=1000
1000+0 records in
1000+0 records out
1000000000 bytes (1.0 GB) copied, 1.80597 s, 554 MB/s
$ qemu-img info test.dd //可以看到dd产生的格式也是raw格式的，并且没有用到稀疏存储方式
image: test.dd
file format: raw
virtual size: 954M (1000000000 bytes)
disk size: 954M
</code></p>

<ul>
<li>（4） convert [-c] [-f fmt] [-O output_fmt] [-o options] filename [filename2 [...]] output_filename</li>
</ul>


<p>镜像格式转换，将fmt格式的filename镜像文件根据options选项转换为格式为<code>output_fmt</code>的名为<code>output_filename</code>的镜像文件。它支持不同格式的镜像文件之间的转换，比如可以用VMware用的vmdk格式文件转换为qcow2文件，这对从其他虚拟化方案转移到KVM上的用户非常有用。一般来说，输入文件格式fmt由qemu-img工具自动检测到，而输出文件格式output_fmt根据自己需要来指定，默认会被转换为与raw文件格式（且默认使用稀疏文件的方式存储以节省存储空间）。
其中，<code>“-c”</code>参数是对输出的镜像文件进行压缩，不过只有qcow2和qcow格式的镜像文件才支持压缩，而且这种压缩是只读的，如果压缩的扇区被重写，则会被重写为未压缩的数据。同样可以使用<code>“-o options”</code>来指定各种选项，如：后端镜像、文件大小、是否加密等等。使用<code>backing_file</code>选项来指定后端镜像，让生成的文件是<code>copy-on-write</code>的增量文件，这时必须让转换命令中指定的后端镜像与输入文件的后端镜像的内容是相同的，尽管它们各自后端镜像的目录、格式可能不同。</p>

<p>如果使用<code>qcow2</code>、<code>qcow</code>、<code>cow</code>等作为输出文件格式来转换<code>raw</code>格式的镜像文件（非稀疏文件格式），镜像转换还可以起到将镜像文件转化为更小的镜像，因为它可以将空的扇区删除使之在生成的输出文件中并不存在。</p>

<p><code>bash
$ qemu-img info test.dd
image: test.dd
file format: raw
virtual size: 954M (1000000000 bytes)
disk size: 954M
$ qemu-img convert -O qcow2  test.dd test_qcow2.qcow2
$ qemu-img info test_qcow2.qcow2
image: test_qcow2.qcow2
file format: qcow2
virtual size: 954M (1000000000 bytes)
disk size: 136K
cluster_size: 65536
</code></p>

<p>以上介绍了<code>qemu-img</code>的基本使用方法之后，关于<code>qemu-img</code>的更多高级用法可以参考man手册</p>

<h3>qemu-kvm</h3>

<h4>新建测试镜像</h4>

<p>因为qcow2的一些特性，这里采用qcow2格式制作系统镜像</p>

<p>``` bash</p>

<h1>qemu-img create -f qcow2 CentOS6.4-x86_64-tpl.qcow2 8G</h1>

<h1>chown oneadmin:oneadmin CentOS6.4-x86_64-tpl.qcow2</h1>

<p>Formatting 'CentOS6.4-x86_64-tpl.qcow2', fmt=qcow2 size=8589934592 encryption=off cluster_size=65536
```</p>

<h4>安装系统</h4>

<p>``` bash</p>

<h1>/usr/libexec/qemu-kvm -m 1024 -cdrom /data/images/CentOS-6.4-x86_64-bin-DVD1.iso -drive \</h1>

<p>file=/data/images/CentOS6.4-x86_64-tpl.qcow2,if=virtio -net nic,model=virtio \
-net tap,script=no  -boot d -nographic -vnc :0
```</p>

<p>上面命令参数解释如下：</p>

<pre><code>-m                      指定内存大小
-cdrom                  指定系统iso镜像
-drive file=xx,if=xx    指定硬盘镜像,file=镜像文件名,if=镜像格式类型
-net nic,model=xx       表示网卡配置,model=模拟网卡类型,默认rt18139
-net tap,script=no      虚拟设备，桥接网络,script表启动虚拟机自动执行网络配置脚本，如果不需要启动，写no即可
-boot d                 系统启动顺序,d表示表示cdrom
-nographic              关闭图形输出
-vnc :0                 开启vnc监听
</code></pre>

<p>详细的关于<code>qemu-kvm</code>的参数使用说明请参考man手册。</p>

<p>输入以上命令之后，通过VNC客户端连接按照正常的安装流程安装系统即可。默认VNC端口从5900开始。</p>

<h2>桥接网络</h2>

<p>如果虚拟机需要和外界通信，则需要创建桥接网络，之前介绍<code>qemu-kvm</code>安装时提到<code>-net tap,script=no</code>选项，默认只是创建桥接虚拟网络，并没有启用，只有启用了才可以设置对应网络配置和外界通信。</p>

<h3>手动桥接</h3>

<p>``` bash</p>

<h1>ip link show dev tap0         //使用如上方式默认创建虚拟网卡tap0，状态为DOWN</h1>

<p>37: tap0: &lt;BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 500</p>

<pre><code>link/ether d2:b0:af:7b:23:0f brd ff:ff:ff:ff:ff:ff
</code></pre>

<h1>brctl show br0                //查看桥接列表，没有tap0</h1>

<p>bridge name   bridge id       STP enabled interfaces
br0       8000.b8975a626020   no      eth0
```</p>

<p>通过以下方式桥接网络</p>

<p>``` bash</p>

<h1>ip link set tap0 up           //使tap0状态变为up</h1>

<h1>brctl addif br0 tap0          //桥接tap0到br0</h1>

<h1>brctl show br0                //显示tap0已经加入到桥接列表</h1>

<p>bridge name   bridge id       STP enabled interfaces
br0       8000.b8975a626020   no      eth0</p>

<pre><code>                                 tap0
</code></pre>

<p>```</p>

<p>如此，配置好虚拟机的网络就可以和外界通信了。</p>

<p><code>brctl delif br0 tap0</code>删除桥接网络，<code>qemu-kvm</code>工具在客户机关闭时会自动解除TAP设备的bridge绑定，所以这一步无需操作。</p>

<h3>脚本实现</h3>

<p>如果不想每次都手动操作，则可以通过脚本自动化实现虚拟网卡的桥接。使用选项<code>-net tap,script=/tmp/qemu-ifup.sh</code> 把之前的no替换为需要执行的脚本，以下为<code>qemu-ifup.sh</code>脚本内容</p>

<p>``` bash</p>

<h1>cat /tmp/qemu-ifup.sh</h1>

<h1>!/bin/bash</h1>

<h1>桥接网络设备</h1>

<p>switch=br0                  //设置桥接网卡</p>

<p>if [ -n $1 ]; then          //$1为qemu-kvm传递值，这里是tap</p>

<pre><code>ip link set $1 up
brctl addif ${switch} $1
exit 0
</code></pre>

<p>else
   echo "no interface!"
   exit 1
fi
```</p>

<h2>系统相关优化</h2>

<p>完成系统安装配置之后，需要对镜像模板系统做如下一系列优化操作：</p>

<h3>selinux、iptables、服务、文件描述符设置</h3>

<p># 关闭SELINUX</p>

<pre><code>sed -i -c 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
</code></pre>

<p># iptables根据相关需求配置，一般关闭iptables服务</p>

<p># 关闭系统其它额外的服务</p>

<p><code>`` bash
service=</code>chkconfig --list | grep '3:on' | awk '{print $1}'`
for i in $service
do</p>

<pre><code>case $i in
    acpid | crond | irqbalance |  messagebus | network | sshd | rsyslog | udev-post)
        chkconfig --level 2345 $i on
    ;;
    *)
        chkconfig --level 2345 $i off   
    ;;
esac    
</code></pre>

<p>done
```</p>

<p># 文件描述符相关配置</p>

<p><code>bash
cat &gt;&gt;/etc/security/limits.conf &lt;&lt;EOF
*               soft    nofile          65535
*               hard    nofile          65535
EOF
sed -i '/1024/s/1024/65535/' /etc/security/limits.d/90-nproc.conf
</code></p>

<blockquote><p><strong>注：</strong> 经测试acpid服务必须安装且在虚拟机系统中开启，否则OpenNebula web端和shell终端发送关机命令无效。</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[grub install with kvm virtio disk]]></title>
    <link href="http://kumu-Linux.github.io/blog/2014/03/26/grub-virtio/"/>
    <updated>2014-03-26T14:31:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2014/03/26/grub-virtio</id>
    <content type="html"><![CDATA[<p>grub-install ERROR: not suitable driver was found</p>

<!--more-->


<ul>
<li><p>First
<code>bash
grub-install /dev/vda
</code></p></li>
<li><p>Second
``` bash</p>

<h1>grub</h1>

<blockquote><p>device (hd0) /dev/vda
root (hd0,0)
setup (hd0)
quit
```</p></blockquote></li>
<li><p><code>/boot/grub/device.map</code>
<code>bash
(hd0) /dev/vda
</code></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ssh X11Forawarding占用OpenNebula kvm端口]]></title>
    <link href="http://kumu-Linux.github.io/blog/2014/03/24/opennebula-port/"/>
    <updated>2014-03-24T17:04:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2014/03/24/opennebula-port</id>
    <content type="html"><![CDATA[<p>虚拟机重建出现如下错误：
<code>bash
$ virsh create deployment.0
error: Failed to create domain from deployment.0
error: Unable to read from monitor: Connection reset by peer
</code></p>

<!--more-->


<p>通过日志发现kvm端口被ssh占用
``` bash</p>

<h1>tail -f /var/log/libvirt/qemu/one-120.log</h1>

<p>… …
inet_listen_opts: bind(ipv4,0.0.0.0,6010): Address already in use
… …</p>

<h1>netstat -tulnp | grep 6010</h1>

<p>tcp        0      0 127.0.0.1:6010          0.0.0.0:*               LISTEN      28575/5</p>

<h1>ps -ef | grep 28575</h1>

<p>root     28575  0.0  0.0  73484  3656 ?        Ss   13:03   0:00 sshd: root@pts/5
```</p>

<p>后查明原因是sshd_config配置文件默认开启<code>X11Forawarding</code>，<code>X11Forawarding</code>默认占用端口为6010开始，和OpenNebula kvm端口占用有冲突。另外，如果连接工具没有开启<code>X11Forawarding</code>，Server端也不会开启转发占用端口的，根本解决方法就是禁用<code>X11Forawarding</code>。</p>

<p>``` bash</p>

<h1>grep X11Forwarding /etc/ssh/sshd_config</h1>

<p>X11Forwarding no</p>

<h1>/etc/init.d/sshd restart</h1>

<p>```</p>

<p>退出已登录的ssh连接，重新登录就不会出现端口占用的问题了，推荐默认关闭X11Forawarding，这在之前的文章<a href="http://kumu-linux.github.io/blog/2013/09/26/ssh-safe/">ssh的一些安全设定</a>已经介绍过了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[opennebula 3.8 market兼容问题]]></title>
    <link href="http://kumu-Linux.github.io/blog/2014/02/20/opennebula-market/"/>
    <updated>2014-02-20T16:12:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2014/02/20/opennebula-market</id>
    <content type="html"><![CDATA[<p>今天下午发现公司所有的OpenNebula Sunstone访问出错，而且错误界面关闭不了，严重影响操作</p>

<!--more-->




<center><img src="http://kumu-Linux.github.io/images/OpenNebula/OpenNebula_market_err.png" /></center>


<p>Firebug获知锁定错误为market插件</p>

<center><img src="http://kumu-Linux.github.io/images/OpenNebula/OpenNebula_market_err2.png" /></center>


<p>Google获知原因所在，具体问题可以参见<a href="http://www.marshut.com/iqmmzn/opennebula-3-8-and-market-compatiable-problem.html">opennebula-3-8-and-market-compatiable-problem</a> ，解决方法是关闭sunstone的Market plugins，修改<code>etc/sunstone-plugins.yaml</code>配置文件，修改下行从<code>True</code>变为<code>false</code>，如下：
``` ruby
- plugins/marketplace-tab.js:</p>

<pre><code>:ALL: false
:user:
</code></pre>

<p>:group:
```</p>

<p>完成之后刷新页面即可恢复正常</p>

<p>--EOF--</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenNebula4.4入门之安装和节点配置]]></title>
    <link href="http://kumu-Linux.github.io/blog/2013/12/19/opennebula4-dot-4/"/>
    <updated>2013-12-19T10:51:00+08:00</updated>
    <id>http://kumu-Linux.github.io/blog/2013/12/19/opennebula4-dot-4</id>
    <content type="html"><![CDATA[<p>OpenNebula入门的PDF文档已可下载，<a href="http://vdisk.weibo.com/s/EKoLFfHiE-oT/1387002741">OpenNebula4.4入门配置</a>，本博客连载更新相关内容</p>

<!--more-->


<p><strong>本文内容目录组成如下</strong>：</p>

<ul>
<li><a href="#env">环境说明</a></li>
<li><a href="#soft">软件包组成</a></li>
<li><a href="#server">Server端安装和配置</a></li>
<li><a href="#node_server">节点端安装配置</a></li>
<li><a href="#node_add">添加节点</a>

<ul>
<li><a href="#onehost">onehost</a></li>
</ul>
</li>
</ul>


<h2 id="env">环境说明</h2>


<p>因为CentOS6.4虚拟化有很大的一个提升，所以系统环境管理端和节点宿主机都采用CentOS6.4 x86_64</p>

<h2 id="soft">软件包组成</h2>


<p>从OpenNebula官网下载<a href="http://downloads.opennebula.org/packages/opennebula-4.4.0/CentOS-6/CentOS-6-opennebula-4.4.0-1.tar.gz">CentOS/RHEL 6</a>对应软件包或者加入OpenNebula源，直接下载软件包这里不再赘述，添加OpenNebula源方法如下：
``` bash</p>

<h1>cat &lt;&lt; EOT > /etc/yum.repos.d/opennebula.repo</h1>

<p>[opennebula]
name=opennebula
baseurl=http://downloads.opennebula.org/repo/CentOS/6/stable/\$basearch
enabled=1
gpgcheck=0
EOT
```</p>

<p>OpenNebula4.4主要有以下几个软件组成：
``` bash</p>

<h1>ls opennebula-*</h1>

<p>opennebula-4.4.0-1.x86_64.rpm           //OpenNebula命令行指令
opennebula-flow-4.4.0-1.x86_64.rpm      //管理OpenNebula服务
opennebula-java-4.4.0-1.x86_64.rpm      //OpenNebula Java Api
opennebula-ozones-4.4.0-1.x86_64.rpm    //OpenNebula网页使用界面
opennebula-server-4.4.0-1.x86_64.rpm    //OpenNebula Server守护进程
opennebula-common-4.4.0-1.x86_64.rpm    //基本依赖性组件
opennebula-gate-4.4.0-1.x86_64.rpm      //使虚拟机和OpenNebula之间的通信
opennebula-node-kvm-4.4.0-1.x86_64.rpm  //元软件包，包括安装oneadmin用户、libvirt和kvm
opennebula-ruby-4.4.0-1.x86_64.rpm      //ruby依赖性组件
opennebula-sunstone-4.4.0-1.x86_64.rpm  //OpenNebula网页使用界面
opennebula-context-4.4.0-1.x86_64.rpm   //context组件
```</p>

<h2 id="server">Server端安装和配置</h2>


<p>为解决一些依赖关系，安装之前可以激活epel源，因为测试为CentOS6.4，因此激活方式如下：
``` bash</p>

<h1>rpm -ivh http://dl.fedoraproject.org/pub/epel/6Server/x86_64/epel-release-6-8.noarch.rpm</h1>

<p>```</p>

<p>如果下载的是OpenNebula软件包，则进入解压目录，安装方式如下 [以下安装为组成Server端最基本的软件]：
``` bash</p>

<h1>yum localinstall opennebula-server-4.4.0-1.x86_64.rpm  \</h1>

<p>opennebula-4.4.0-1.x86_64.rpm opennebula-common-4.4.0-1.x86_64.rpm \
opennebula-ruby-4.4.0-1.x86_64.rpm opennebula-sunstone-4.4.0-1.x86_64 -y
```</p>

<p>如果使用OpenNebula的源，安装如下：
``` bash</p>

<h1>yum install opennebula-server opennebula-sunstone -y</h1>

<p>```</p>

<p>安装完成之后创建如下用户以及目录文件：
``` bash</p>

<h1>grep oneadmin /etc/passwd</h1>

<p>oneadmin:x:9869:9869::/var/lib/one:/bin/bash</p>

<h1>ls -ld /etc/one/  //OpenNebula相关配置文件所在目录</h1>

<p>drwxr-x---. 11 root oneadmin 4096 Aug 20 11:35 /etc/one/</p>

<h1>ls /etc/init.d/opennebula*</h1>

<p>/etc/init.d/opennebula  /etc/init.d/opennebula-occi  /etc/init.d/opennebula-sunstone</p>

<h1>ls -ld /var/log/one/</h1>

<p>drwxr-x---. 2 oneadmin oneadmin 4096 Jul 25 01:13 /var/log/one/
```</p>

<p>默认OpenNebula数据存储使用sqlite，如果需要使用MySQL，则需要做如下操作：<br/>
<strong>1.</strong>  创建相关数据库：
``` bash
mysql> create database opennebula;
Query OK, 1 row affected (0.00 sec)</p>

<p>mysql> grant all privileges on opennebula.* to oneadmin@'localhost' identified by 'oneadmin';
Query OK, 0 rows affected (0.00 sec)</p>

<p>mysql> flush privileges;
Query OK, 0 rows affected (0.00 sec)
```</p>

<p><strong>2.</strong>  修改配置文件如下 [用户、端口、密码、库名和实际情况对应修改]：
``` bash</p>

<h1>vim /etc/one/oned.conf</h1>

<p>… …</p>

<h1>DB = [ backend = "sqlite" ]</h1>

<h1>Sample configuration for MySQL</h1>

<p>DB = [ backend = "mysql",</p>

<pre><code>   server  = "localhost",
   port    = 3306,
   user    = "oneadmin", 
   passwd  = "oneadmin", 
   db_name = "opennebula" ]
</code></pre>

<p>… …
```</p>

<p>修改sunstone默认监听IP:
``` bash</p>

<h1>grep ':host' /etc/one/sunstone-server.conf</h1>

<p>:host: 127.0.0.1</p>

<h1>sed -i '/:host/s/127.0.0.1/192.168.80.130/g' /etc/one/sunstone-server.conf</h1>

<h1>grep ':host' /etc/one/sunstone-server.conf</h1>

<p>:host: 192.168.80.130
```</p>

<p>启动相关服务:
``` bash</p>

<h1>/etc/init.d/opennebula start</h1>

<h1>/etc/init.d/opennebula-sunstone start</h1>

<h1>lsof -i:9869</h1>

<p>COMMAND   PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
ruby    22266 oneadmin    6u  IPv4 106746      0t0  TCP 192.168.80.130:9869 (LISTEN)
```</p>

<p>修改datastore:</p>

<p>OpenNebula默认用的是Shared Transfer Driver，这种模式比较适合快速部署和热迁移，只是要配置网络文件系统。如果没有网络文件系统，不想做热迁移，那么可以换成SSH Transfer Driver测试部署。
<code>bash
$ onedatastore list
  ID NAME                SIZE AVAIL CLUSTER      IMAGES TYPE DS       TM      
   0 system                0M -     -                 0 sys  -        shared
   1 default            98.4G 85%   -                 1 img  fs       shared
   2 files              98.4G 85%   -                 0 fil  fs       ssh
$ onedatastore update 1
CLONE_TARGET="SYSTEM"
DISK_TYPE="FILE"
DS_MAD="fs"
LN_TARGET="SYSTEM"
TM_MAD="ssh"
TYPE="IMAGE_DS"
$ onedatastore list
  ID NAME                SIZE AVAIL CLUSTER      IMAGES TYPE DS       TM      
   0 system                0M -     -                 0 sys  -        shared
   1 default            98.4G 85%   -                 1 img  fs       ssh
   2 files              98.4G 85%   -                 0 fil  fs       ssh
</code></p>

<p>修改过程产生如下错误：
<code>bash
$ onedatastore update 1
Editor not defined
</code></p>

<p>这是因为如下原因，CentOS默认vi位置是/bin/vi，添加相关链接即可
``` bash</p>

<h1>grep -i editor_path= /usr/lib/one/ruby/cli/one_helper.rb</h1>

<p>EDITOR_PATH='/usr/bin/vi'</p>

<h1>ln -s /bin/vi /usr/bin/vi</h1>

<p>```</p>

<p>用户名和密码通过以下方式获得：
``` bash</p>

<h1>cat /var/lib/one/.one/one_auth</h1>

<p>oneadmin:cd24c3a59c9fd8a7ab853b10247e8147
```</p>

<p><strong>注</strong>：测试过程中因为测试环境服务端时间不对，导致cookie被忽略，OpenNebula Sunstone选择Keep me logged in一直登陆不上或者直接登陆很快退出，寻找原因花了很长时间，最后调整到正确时间，登陆显示ok。P.S: 时间是一个非常容易被我们忽略的问题，切记切记!</p>

<p>完成以上步骤之后，浏览器登陆 <a href="http://ip:9869">http://ip:9869</a> 即可</p>

<h2 id="node_server">节点端安装配置</h2>


<p>软件包下载见Server端安装章节，节点只需要安装以下两个软件</p>

<pre><code>opennebula-node-kvm-4.4.0-1.x86_64.rpm  
opennebula-common-4.4.0-1.x86_64.rpm
</code></pre>

<p>yum安装以上软件即可，安装过程同时会安装虚拟化相关组件，包括bridge-utils、libvirt、qemu-kvm、qemu-img等。</p>

<p>桥接网络：
``` bash</p>

<h1>cat /etc/sysconfig/network-scripts/ifcfg-eth0</h1>

<p>DEVICE=eth0
TYPE=Ethernet
ONBOOT=yes
BRIDGE=br0
NAME="System eth0"</p>

<h1>cat /etc/sysconfig/network-scripts/ifcfg-br0</h1>

<p>DEVICE=br0
ONBOOT=yes
TYPE=Bridge
BOOTPROTO=static
IPADDR=192.168.80.131
NETMASK=255.255.255.0
GATEWAY=192.168.80.2
```</p>

<p>修改之后，重启网络并查看确认：
``` bash</p>

<h1>service network restart</h1>

<h1>brctl show</h1>

<p>bridge name bridge id       STP enabled interfaces
br0     8000.000c2942e561   no      eth0
```</p>

<p>修改/etc/libvirt/qemu.conf的相关配置：
<code>bash
user  = "oneadmin"
group = "oneadmin"
dynamic_ownership = 0
</code></p>

<p>修改/etc/libvirt/libvirtd.conf相关配置：
<code>bash
listen_tcp = 1          //OpenNebula使用libvirt提供的TCP协议
listen_tls = 0
</code></p>

<p>修改/etc/sysconfig/libvirtd开启监听选项：
<code>bash
LIBVIRTD_ARGS="--listen"
</code></p>

<p>启动libvirtd服务：
``` bash</p>

<h1>/etc/init.d/libvirtd start</h1>

<h1>netstat -tulnp | grep libvirt</h1>

<p>tcp        0      0 0.0.0.0:16509               0.0.0.0:*                   LISTEN      2664/libvirtd
```</p>

<p>ssh无密码登陆：</p>

<p>ssh使用公钥认证无密码登陆这个比较简单，顺带也提一下，方法如下：</p>

<p><strong>管理端</strong>
``` bash</p>

<h1>su - oneadmin</h1>

<p>$ cat ~/.ssh/config     //增加超时时间，不询问直接添加主机到known_hosts文件
ConnectTimeout 5
Host *</p>

<pre><code>StrictHostKeyChecking no
UserKnownHostsFile /dev/null
</code></pre>

<p>```</p>

<p><strong>节点端</strong>
``` bash</p>

<h1>su - oneadmin</h1>

<p>$ vim .ssh/authorized_keys          //把管理端ssh公钥加入节点.ssh/authorized_keys文件
$ chmod 400 .ssh/authorized_keys
```</p>

<p>如此，Server端的oneadmin用户就可以无密码登陆节点oneadmin了。</p>

<h2 id="node_add">添加节点</h2>


<p>节点如此安装软件和配置之后便可以在Server端添加了，可以使用web添加，也可以使用命令添加。关于web界面的添加可以参考本人共享的pdf文档，这里不作具体的介绍，只介绍命令添加。</p>

<h3 id="onehost">onehost命令</h3>


<p>使用命令行添加主机也比较简单，这里使用的命令是<strong>onehost</strong></p>

<p>使用onehost命令删除之前web创建的主机，如下：
<code>bash
$ su - oneadmin
$ onehost list
  ID NAME            CLUSTER   RVM      ALLOCATED_CPU      ALLOCATED_MEM STAT  
   1 192.168.80.131  -           0       0 / 400 (0%)     0K / 3.7G (0%) on    
$ onehost delete 1      //删除主机，可以是ID也可以是NAME
$ onehost list
  ID NAME            CLUSTER   RVM      ALLOCATED_CPU      ALLOCATED_MEM static
</code></p>

<p>当然删除之后我们还是需要再创建一遍，虽然很无聊，But你懂的，如下
<code>bash
$ onehost create 192.168.80.131 --im kvm --vm kvm --net dummy
ID: 2
$ onehost list
  ID NAME            CLUSTER   RVM      ALLOCATED_CPU      ALLOCATED_MEM STAT  
   2 192.168.80.131  -           0       0 / 400 (0%)     0K / 3.7G (0%) on    
</code></p>

<pre><code>--im/-i:信息管理driver. 可选: kvm, xen, vmware, ec2, ganglia, dummy.
--vm/-v: 虚拟化管理driver. 可选: kvm, xen, vmware, ec2, dummy.
--net/-n: 虚拟网络driver. 可选: 802.1Q,dummy,ebtables,fw,ovswitch,vmware.
</code></pre>

<p>查看主机的详细信息 <strong>onehost show</strong>
```
$ onehost show 2
HOST 2 INFORMATION                                                            <br/>
ID                    : 2                 <br/>
NAME                  : 192.168.80.131    <br/>
CLUSTER               : -                 <br/>
STATE                 : MONITORED         <br/>
IM_MAD                : kvm               <br/>
VM_MAD                : kvm               <br/>
VN_MAD                : dummy             <br/>
LAST MONITORING TIME  : 11/29 22:19:21</p>

<p>HOST SHARES                                                                   <br/>
TOTAL MEM             : 3.7G              <br/>
USED MEM (REAL)       : 111M              <br/>
USED MEM (ALLOCATED)  : 0K                <br/>
TOTAL CPU             : 400               <br/>
USED CPU (REAL)       : 0                 <br/>
USED CPU (ALLOCATED)  : 0                 <br/>
RUNNING VMS           : 0</p>

<p>… …
```</p>

<p>通过-x选项还可以以xml的格式显示主机相关信息
```
$ onehost show -x 2
<HOST>
  <ID>2</ID>
  <NAME>192.168.80.131</NAME>
  <STATE>2</STATE>
  <IM_MAD>kvm</IM_MAD>
  <VM_MAD>kvm</VM_MAD>
  <VN_MAD>dummy</VN_MAD>
  <LAST_MON_TIME>1385735001</LAST_MON_TIME>
  <CLUSTER_ID>-1</CLUSTER_ID>
  <CLUSTER/>
  <HOST_SHARE></p>

<pre><code>&lt;DISK_USAGE&gt;0&lt;/DISK_USAGE&gt;
&lt;MEM_USAGE&gt;0&lt;/MEM_USAGE&gt;
&lt;CPU_USAGE&gt;0&lt;/CPU_USAGE&gt;
&lt;MAX_DISK&gt;0&lt;/MAX_DISK&gt;
&lt;MAX_MEM&gt;3916984&lt;/MAX_MEM&gt;
&lt;MAX_CPU&gt;400&lt;/MAX_CPU&gt;
&lt;FREE_DISK&gt;0&lt;/FREE_DISK&gt;
&lt;FREE_MEM&gt;3803128&lt;/FREE_MEM&gt;
&lt;FREE_CPU&gt;399&lt;/FREE_CPU&gt;
&lt;USED_DISK&gt;0&lt;/USED_DISK&gt;
&lt;USED_MEM&gt;113856&lt;/USED_MEM&gt;
&lt;USED_CPU&gt;0&lt;/USED_CPU&gt;
&lt;RUNNING_VMS&gt;0&lt;/RUNNING_VMS&gt;
</code></pre>

<p>  </HOST_SHARE>
… …
```</p>

<p>onehost还有两个选项,disable和enable，disable表示不再监控该物理主机，但是不影响正在运行的虚拟机，enable表示开启监控
<code>bash
$ onehost disable 0
$ onehost enable 0
</code></p>

<p>--EOF--</p>
]]></content>
  </entry>
  
</feed>
